# Machine Learning Pipeline Documentation

## ğŸ“‹ Tá»•ng quan

**Má»¥c tiÃªu**: XÃ¢y dá»±ng model phÃ¢n loáº¡i job categories tá»± Ä‘á»™ng tá»« job posting data (job title, description, skills).

**BÃ i toÃ¡n**: Multi-class Classification
- **Input**: Job posting (text + metadata)
- **Output**: Job category (10 categories)
- **Metric chÃ­nh**: Accuracy, Precision, Recall, F1-Score

**Dataset**: `jobs_master.csv` (3,985 jobs) â†’ Filtered to **3,859 jobs** (96.8%) cho ML

---

## ğŸ¯ ML Pipeline Overview

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  jobs_master    â”‚ (3,985 jobs, 19 columns)
â”‚  (Full dataset) â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         â”œâ”€ Step 1: Filter Categories (â‰¥50 samples)
         â”‚
         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  ML Dataset     â”‚ (3,859 jobs, 10 categories)
â”‚  (96.8% data)   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         â”œâ”€ Step 2: Build Text Feature
         â”‚  (job_title + job_description + skills)
         â”‚
         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Combined Text  â”‚ (3,859 text documents)
â”‚                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         â”œâ”€ Step 3: TF-IDF Vectorization
         â”‚  (500 features, bigrams, stopwords removed)
         â”‚
         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  TF-IDF Matrix  â”‚ (3,859 Ã— 500 sparse matrix)
â”‚                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         â”œâ”€ Step 4: Add Numeric Features
         â”‚  (level_encoded, city_encoded, has_salary, etc.)
         â”‚
         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Feature Matrix â”‚ (3,859 Ã— 505 features)
â”‚  (X)            â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         â”œâ”€ Step 5: Train/Test Split (80/20, stratified)
         â”‚
         â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
         â–¼              â–¼              â–¼
   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”
   â”‚  Train  â”‚   â”‚  Test   â”‚   â”‚ Labels  â”‚
   â”‚ (3,087) â”‚   â”‚  (772)  â”‚   â”‚   (y)   â”‚
   â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”˜
         â”‚              â”‚             â”‚
         â”œâ”€ Step 6: Train Models      â”‚
         â”‚  - Random Forest           â”‚
         â”‚  - XGBoost                 â”‚
         â”‚                            â”‚
         â–¼                            â–¼
   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
   â”‚  Models â”‚              â”‚  Predictions â”‚
   â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”˜              â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜
         â”‚                          â”‚
         â”œâ”€ Step 7: Evaluate â”€â”€â”€â”€â”€â”€â”€â”¤
         â”‚  - Accuracy               â”‚
         â”‚  - Classification Report  â”‚
         â”‚  - Confusion Matrix       â”‚
         â”‚                            â”‚
         â–¼                            â”‚
   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                   â”‚
   â”‚ Best Model  â”‚ â—„â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
   â”‚  XGBoost    â”‚ (Accuracy: 77%)
   â”‚  (Acc=0.77) â”‚
   â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜
          â”‚
          â”œâ”€ Step 8: Save Model Package
          â”‚
          â–¼
   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
   â”‚  best_model.pkl  â”‚ (Model + TF-IDF + Encoders + Metadata)
   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸ“Š Step-by-Step Process

---

## Step 1: Filter Categories (â‰¥50 samples)

### Má»¥c tiÃªu
Loáº¡i bá» categories cÃ³ quÃ¡ Ã­t samples (imbalanced, khÃ´ng Ä‘á»§ Ä‘á»ƒ train model).

### Quy táº¯c lá»c
- **Threshold**: Giá»¯ láº¡i categories cÃ³ **â‰¥50 jobs**
- **LÃ½ do**: 
  - Categories nhá» (<50) khÃ´ng Ä‘á»§ Ä‘á»ƒ split train/test
  - TrÃ¡nh overfitting trÃªn classes quÃ¡ Ã­t data
  - Äáº£m báº£o má»—i class cÃ³ Ã­t nháº¥t 40 train + 10 test samples

### Input
- **File**: `data/final/jobs_master.csv`
- **Total jobs**: 3,985 jobs
- **Total categories**: 13 categories

### Process
```python
# Count samples per category
category_counts = df_master['job_category'].value_counts()

# Filter categories with â‰¥50 samples
valid_categories = category_counts[category_counts >= 50].index.tolist()

# Filter dataset
df_ml = df_master[df_master['job_category'].isin(valid_categories)].copy()
```

### Output
- **ML Dataset**: 3,859 jobs (96.8% of original data)
- **Valid Categories**: 10 categories

**Category Distribution** (sorted by count):

| Category | Count | Percentage | Status |
|----------|-------|------------|--------|
| Other | 1,615 | 41.8% | âœ… Included |
| Backend Developer | 374 | 9.7% | âœ… Included |
| Fullstack Developer | 345 | 8.9% | âœ… Included |
| QA/Tester | 338 | 8.8% | âœ… Included |
| Mobile Developer | 303 | 7.9% | âœ… Included |
| Frontend Developer | 292 | 7.6% | âœ… Included |
| Software Engineer | 249 | 6.5% | âœ… Included |
| Business Analyst | 185 | 4.8% | âœ… Included |
| DevOps Engineer | 87 | 2.3% | âœ… Included |
| Data Engineer | 71 | 1.8% | âœ… Included |
| **Data Scientist** | **39** | **1.0%** | âŒ Excluded (<50) |
| **Security Engineer** | **45** | **1.1%** | âŒ Excluded (<50) |
| **Product Manager** | **42** | **1.1%** | âŒ Excluded (<50) |

**Imbalance Note**: 
- **Majority class** (Other): 1,615 jobs (41.8%)
- **Minority class** (Data Engineer): 71 jobs (1.8%)
- **Class imbalance ratio**: 22.7:1 (Other vs Data Engineer)
- âš ï¸ **High imbalance** â†’ Model may bias toward "Other" category

---

## Step 2: Build Text Feature

### Má»¥c tiÃªu
Káº¿t há»£p cÃ¡c trÆ°á»ng text thÃ nh 1 document duy nháº¥t cho má»—i job.

### Text Fields Sá»­ Dá»¥ng
1. **`job_title`**: TÃªn vá»‹ trÃ­ (e.g., "Senior Backend Engineer")
2. **`job_description`**: MÃ´ táº£ cÃ´ng viá»‡c (full text, multiple paragraphs)
3. **`skills`**: Danh sÃ¡ch ká»¹ nÄƒng (pipe-separated: "python|django|postgresql")

### Process
```python
# Combine text fields (fillna to handle missing values)
df_ml['text'] = (
    df_ml['job_title'].fillna('') + ' ' + 
    df_ml['job_description'].fillna('') + ' ' + 
    df_ml['skills'].fillna('')
)
```

### Example
**Input**:
- `job_title`: "Senior Backend Engineer"
- `job_description`: "We are looking for a talented Backend Engineer with strong Python and Django experience..."
- `skills`: "python|django|postgresql|docker"

**Output** (combined text):
```
Senior Backend Engineer We are looking for a talented Backend Engineer with strong Python and Django experience... python django postgresql docker
```

### Data Coverage
- **job_title**: 100% (3,859/3,859) - Always available
- **job_description**: ~100% (3,859/3,859) - Always available
- **skills**: 92.7% (3,693/3,985 in full dataset) - Some jobs missing skills

**Handling Missing Values**:
- Use `.fillna('')` to replace NaN with empty string
- Missing skills â†’ text will only contain title + description

---

## Step 3: TF-IDF Vectorization

### Má»¥c tiÃªu
Chuyá»ƒn text thÃ nh numeric features (vector) Ä‘á»ƒ model cÃ³ thá»ƒ há»c.

### TF-IDF Configuration
```python
from sklearn.feature_extraction.text import TfidfVectorizer

tfidf = TfidfVectorizer(
    max_features=500,       # Giá»¯ láº¡i 500 tá»« quan trá»ng nháº¥t
    ngram_range=(1, 2),     # Unigrams (1 tá»«) + Bigrams (2 tá»«)
    stop_words='english'    # Loáº¡i bá» stopwords tiáº¿ng Anh (the, is, and, etc.)
)

X_tfidf = tfidf.fit_transform(df_ml['text'])
```

### Parameters Explained

**`max_features=500`**:
- Giá»¯ láº¡i 500 features quan trá»ng nháº¥t (theo TF-IDF score)
- LÃ½ do: Balance giá»¯a information vÃ  computational cost
- Alternative: 1000 features (more info, slower), 200 features (faster, less info)

**`ngram_range=(1, 2)`**:
- **Unigrams** (1 word): `"python"`, `"senior"`, `"backend"`
- **Bigrams** (2 words): `"backend engineer"`, `"machine learning"`, `"react native"`
- LÃ½ do: Bigrams capture domain-specific terms (e.g., "data engineer" vs "data" + "engineer")

**`stop_words='english'`**:
- Loáº¡i bá» common words khÃ´ng mang Ã½ nghÄ©a: `the`, `is`, `and`, `of`, `to`, etc.
- LÃ½ do: Giáº£m noise, focus vÃ o technical terms
- Note: Chá»‰ loáº¡i stopwords tiáº¿ng Anh (dataset cÃ³ mixed Vietnamese/English)

### Output
- **Matrix Shape**: (3,859 jobs Ã— 500 features)
- **Matrix Type**: Sparse matrix (scipy.sparse.csr_matrix)
- **Sparsity**: ~95-98% (most values are 0)
- **Storage**: Efficient (only store non-zero values)

### Example Features (Top TF-IDF terms)
Typical high-scoring terms:
- Technical skills: `python`, `java`, `javascript`, `react`, `sql`
- Job-specific: `engineer`, `developer`, `senior`, `junior`
- Bigrams: `backend developer`, `machine learning`, `full stack`, `react native`
- Domain terms: `api`, `database`, `cloud`, `devops`, `testing`

---

## Step 4: Add Numeric Features

### Má»¥c tiÃªu
Bá»• sung features tá»« metadata columns (khÃ´ng pháº£i text).

### Feature List (5 features)

**1. `level_encoded`** (int)
- **Source**: `job_level` column
- **Encoding**: LabelEncoder
- **Values**: 
  - 0 = intern
  - 1 = junior
  - 2 = mid
  - 3 = senior
  - 4 = manager
- **Rationale**: Job level cÃ³ thá»ƒ dá»± Ä‘oÃ¡n category (e.g., senior â†’ Backend, junior â†’ QA)

**2. `city_encoded`** (int)
- **Source**: `city` column
- **Encoding**: LabelEncoder
- **Values**: 
  - 0 = Ha Noi
  - 1 = Ho Chi Minh
  - 2 = Da Nang
  - 3 = Remote
  - ... (9 cities total)
- **Rationale**: City cÃ³ thá»ƒ liÃªn quan Ä‘áº¿n job type (e.g., HCM nhiá»u Backend, HN nhiá»u Data)

**3. `has_salary`** (binary: 0 or 1)
- **Source**: `salary_avg` column
- **Logic**: `1` if salary is not NULL, `0` if NULL
- **Current status**: All 0 (no salary data in current dataset)
- **Rationale**: Jobs with salary disclosed may differ from jobs without salary

**4. `title_length`** (int)
- **Source**: `job_title` column
- **Calculation**: `len(job_title)`
- **Example**: "Senior Backend Engineer" â†’ 23 characters
- **Rationale**: Title length may correlate with seniority (longer titles = more senior?)

**5. `desc_length`** (int)
- **Source**: `job_description` column
- **Calculation**: `len(job_description)` (fillna('') for missing)
- **Range**: 0 to ~5000+ characters
- **Rationale**: Description length may indicate job complexity or company professionalism

### Process
```python
# LabelEncoders
le_level = LabelEncoder()
le_city = LabelEncoder()

# Encode categorical features
df_ml['level_encoded'] = le_level.fit_transform(df_ml['job_level'])
df_ml['city_encoded'] = le_city.fit_transform(df_ml['city'])

# Binary feature
df_ml['has_salary'] = df_ml['salary_avg'].notna().astype(int)

# Length features
df_ml['title_length'] = df_ml['job_title'].str.len()
df_ml['desc_length'] = df_ml['job_description'].fillna('').str.len()

# Extract features
feature_cols = ['level_encoded', 'city_encoded', 'has_salary', 'title_length', 'desc_length']
X_additional = df_ml[feature_cols].values
```

### Combine with TF-IDF
```python
from scipy.sparse import hstack

# X_tfidf: (3,859 Ã— 500) sparse matrix
# X_additional: (3,859 Ã— 5) dense array

X = hstack([X_tfidf, X_additional])
# X: (3,859 Ã— 505) sparse matrix
```

### Final Feature Matrix
- **Shape**: (3,859 jobs Ã— 505 features)
- **Composition**:
  - 500 TF-IDF features (text)
  - 5 numeric features (metadata)
- **Type**: Sparse matrix (scipy.sparse.csr_matrix)

---

## Step 5: Train/Test Split

### Má»¥c tiÃªu
Chia dá»¯ liá»‡u thÃ nh train set (Ä‘á»ƒ há»c) vÃ  test set (Ä‘á»ƒ Ä‘Ã¡nh giÃ¡).

### Configuration
```python
from sklearn.model_selection import train_test_split

X_train, X_test, y_train, y_test = train_test_split(
    X,                      # Features (3,859 Ã— 505)
    y,                      # Labels (3,859,)
    test_size=0.2,          # 20% test, 80% train
    random_state=42,        # Reproducible split
    stratify=y              # Maintain class distribution
)
```

### Parameters Explained

**`test_size=0.2`**:
- **Train**: 80% = 3,087 jobs
- **Test**: 20% = 772 jobs
- **Rationale**: Standard ML split (common: 80/20 or 70/30)

**`random_state=42`**:
- Fixed seed for reproducibility
- Same split every time you run the code
- Important for comparing different models fairly

**`stratify=y`**:
- **Critical for imbalanced data**
- Ensures each class has same proportion in train and test
- Example: If "Other" is 41.8% overall â†’ 41.8% in train AND 41.8% in test

### Output Sizes
| Set | Samples | Features | Labels |
|-----|---------|----------|--------|
| **Train** | 3,087 (80%) | 505 | 10 classes |
| **Test** | 772 (20%) | 505 | 10 classes |

### Stratification Example
Without stratification (bad):
- Train: "Other" 50%, "Data Engineer" 0.5%
- Test: "Other" 30%, "Data Engineer" 5%
- Problem: Test distribution â‰  Train distribution

With stratification (good):
- Train: "Other" 41.8%, "Data Engineer" 1.8%
- Test: "Other" 41.8%, "Data Engineer" 1.8%
- âœ… Same distribution in both sets

---

## Step 6: Model Training

### Models Tested

**1. Random Forest Classifier**
```python
from sklearn.ensemble import RandomForestClassifier

rf_model = RandomForestClassifier(
    n_estimators=100,    # 100 decision trees
    random_state=42,     # Reproducibility
    n_jobs=-1            # Use all CPU cores
)

rf_model.fit(X_train, y_train)
```

**Configuration**:
- **n_estimators=100**: Ensemble of 100 decision trees
- **random_state=42**: Reproducible results
- **n_jobs=-1**: Parallel training (use all CPU cores)

**Characteristics**:
- âœ… Handles high-dimensional sparse data well
- âœ… Robust to overfitting (ensemble method)
- âœ… No need for feature scaling
- âŒ Slower training than simple models
- âŒ Large model size (100 trees)

**Training Time**: ~2-5 minutes (depending on CPU)

---

**2. XGBoost Classifier**
```python
from xgboost import XGBClassifier

xgb_model = XGBClassifier(
    n_estimators=100,           # 100 boosting rounds
    random_state=42,            # Reproducibility
    eval_metric='mlogloss'      # Multi-class log loss
)

xgb_model.fit(X_train, y_train)
```

**Configuration**:
- **n_estimators=100**: 100 gradient boosting iterations
- **eval_metric='mlogloss'**: Optimization metric for multi-class
- **random_state=42**: Reproducible results

**Characteristics**:
- âœ… State-of-the-art gradient boosting
- âœ… Better accuracy than Random Forest
- âœ… Handles imbalanced data well
- âœ… Feature importance available
- âŒ Slower inference than simple models
- âŒ Hyperparameter tuning needed for best results

**Training Time**: ~3-7 minutes (depending on CPU)

---

### Training Process
```python
models = {
    'Random Forest': RandomForestClassifier(n_estimators=100, random_state=42, n_jobs=-1),
    'XGBoost': XGBClassifier(n_estimators=100, random_state=42, eval_metric='mlogloss')
}

results = {}

for name, model in models.items():
    print(f'Training {name}...')
    model.fit(X_train, y_train)
    
    # Predict on test set
    y_pred = model.predict(X_test)
    
    # Calculate accuracy
    acc = accuracy_score(y_test, y_pred)
    
    # Store results
    results[name] = {
        'model': model,
        'accuracy': acc,
        'predictions': y_pred
    }
    
    print(f'{name} Accuracy: {acc:.4f}')
```

### Results Summary

| Model | Accuracy | Training Time | Model Size | Best For |
|-------|----------|---------------|------------|----------|
| **Random Forest** | **~0.69** (69%) | ~2-5 min | Large (100 trees) | Baseline, interpretable |
| **XGBoost** | **~0.77** (77%) | ~3-7 min | Medium | **Best accuracy** âœ… |

**Best Model**: **XGBoost** (77% accuracy)

---

## Step 7: Model Evaluation

### Metrics Used

**1. Accuracy**
- **Definition**: `(Correct predictions) / (Total predictions)`
- **Random Forest**: 69%
- **XGBoost**: **77%** âœ…
- **Interpretation**: Model correctly predicts category 77% of the time

**2. Classification Report**
```python
from sklearn.metrics import classification_report

print(classification_report(y_test, y_pred_best, target_names=le_target.classes_))
```

**Output** (per-class metrics):
- **Precision**: `TP / (TP + FP)` - How many predicted X are actually X?
- **Recall**: `TP / (TP + FN)` - How many actual X did we find?
- **F1-Score**: `2 * (Precision * Recall) / (Precision + Recall)` - Harmonic mean
- **Support**: Number of samples in test set

**Example Output**:
```
                      precision    recall  f1-score   support

            Other       0.65      0.85      0.74       323
Backend Developer       0.82      0.75      0.78        75
  Fullstack Developer   0.78      0.68      0.73        69
        QA/Tester       0.88      0.72      0.79        68
   Mobile Developer     0.85      0.73      0.79        61
 Frontend Developer     0.79      0.71      0.75        58
 Software Engineer      0.72      0.60      0.65        50
  Business Analyst      0.83      0.75      0.79        37
    DevOps Engineer     0.90      0.65      0.76        17
     Data Engineer      0.88      0.70      0.78        14

         accuracy                           0.77       772
        macro avg       0.81      0.71      0.75       772
     weighted avg       0.78      0.77      0.77       772
```

**Key Observations**:
- âœ… **High precision categories**: DevOps (90%), QA (88%), Data Engineer (88%)
- âš ï¸ **Low recall categories**: Software Engineer (60%), DevOps (65%), Data Engineer (70%)
- âš ï¸ **"Other" dominance**: Highest support (323), but lower precision (65%)

**3. Confusion Matrix**
```python
from sklearn.metrics import confusion_matrix
import seaborn as sns

cm = confusion_matrix(y_test, y_pred_best)
sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',
            xticklabels=le_target.classes_,
            yticklabels=le_target.classes_)
plt.title('Confusion Matrix - XGBoost')
plt.ylabel('True')
plt.xlabel('Predicted')
plt.show()
```

**Interpretation**:
- **Diagonal values** (correct predictions): High is good
- **Off-diagonal values** (errors): Low is good
- **Common confusions**:
  - Backend â†” Software Engineer (generic vs specific)
  - Fullstack â†” Backend/Frontend (overlapping skills)
  - Other â† Many categories (catch-all misclassifications)

---

### Model Comparison Visualization
```python
plt.figure(figsize=(10, 6))
model_names = ['Random Forest', 'XGBoost']
accuracies = [0.69, 0.77]

plt.bar(model_names, accuracies)
plt.title('Model Accuracy Comparison')
plt.ylabel('Accuracy')
plt.ylim(0, 1)
for i, v in enumerate(accuracies):
    plt.text(i, v + 0.01, f'{v:.4f}', ha='center')
plt.tight_layout()
plt.show()
```

**Result**: XGBoost outperforms Random Forest by **8 percentage points** (77% vs 69%)

---

## Step 8: Save Best Model

### Model Package Contents
```python
model_data = {
    'model': results[best_name]['model'],           # Trained XGBoost model
    'tfidf': tfidf,                                  # Fitted TfidfVectorizer
    'le_target': le_target,                          # Target LabelEncoder (categories)
    'le_level': le_level,                            # Level LabelEncoder
    'le_city': le_city,                              # City LabelEncoder
    'feature_cols': feature_cols,                    # List of numeric feature names
    'accuracy': results[best_name]['accuracy'],      # Best accuracy (0.77)
    'model_name': best_name                          # "XGBoost"
}

with open('data/final/best_model.pkl', 'wb') as f:
    pickle.dump(model_data, f)
```

### Package Components Explained

**1. `model`** (XGBClassifier)
- Trained XGBoost model with learned weights
- Used for prediction: `model.predict(X_new)`

**2. `tfidf`** (TfidfVectorizer)
- Fitted vectorizer (knows vocabulary and IDF values)
- Transform new text: `X_tfidf_new = tfidf.transform(text_new)`
- **Critical**: Must use same vectorizer for inference

**3. `le_target`** (LabelEncoder for categories)
- Maps integer labels â†’ category names
- Example: 0 â†’ "Other", 1 â†’ "Backend Developer"
- Decode predictions: `category = le_target.inverse_transform(y_pred)`

**4. `le_level`** (LabelEncoder for job_level)
- Maps job_level â†’ integers
- Example: "senior" â†’ 3
- Used to encode new job's level

**5. `le_city`** (LabelEncoder for city)
- Maps city â†’ integers
- Example: "Ho Chi Minh" â†’ 1
- Used to encode new job's city

**6. `feature_cols`**
- List: `['level_encoded', 'city_encoded', 'has_salary', 'title_length', 'desc_length']`
- Order matters! Must extract features in same order during inference

**7. `accuracy`**
- Test accuracy: 0.77
- Metadata for model card

**8. `model_name`**
- String: "XGBoost"
- Metadata for tracking

### File Location
- **Path**: `data/final/best_model.pkl`
- **Size**: ~50-100 MB (depending on model complexity)
- **Format**: Pickle (Python serialization)

### Usage Example (Inference)
```python
import pickle
import pandas as pd

# Load model package
with open('data/final/best_model.pkl', 'rb') as f:
    model_data = pickle.load(f)

# Extract components
model = model_data['model']
tfidf = model_data['tfidf']
le_target = model_data['le_target']
le_level = model_data['le_level']
le_city = model_data['le_city']
feature_cols = model_data['feature_cols']

# New job data
new_job = {
    'job_title': 'Senior Backend Engineer',
    'job_description': 'We are looking for a talented Backend Engineer...',
    'skills': 'python|django|postgresql|docker',
    'job_level': 'senior',
    'city': 'Ho Chi Minh'
}

# Combine text
text = new_job['job_title'] + ' ' + new_job['job_description'] + ' ' + new_job['skills']

# Transform text
X_tfidf_new = tfidf.transform([text])

# Numeric features
level_enc = le_level.transform([new_job['job_level']])[0]
city_enc = le_city.transform([new_job['city']])[0]
has_salary = 0  # No salary in input
title_len = len(new_job['job_title'])
desc_len = len(new_job['job_description'])

X_additional_new = [[level_enc, city_enc, has_salary, title_len, desc_len]]

# Combine features
from scipy.sparse import hstack
X_new = hstack([X_tfidf_new, X_additional_new])

# Predict
y_pred = model.predict(X_new)
category = le_target.inverse_transform(y_pred)[0]

print(f'Predicted category: {category}')
# Output: Predicted category: Backend Developer
```

---

## ğŸ“ˆ Performance Summary

### Dataset Statistics
| Metric | Value |
|--------|-------|
| **Total jobs** | 3,985 |
| **Jobs for ML** | 3,859 (96.8%) |
| **Categories** | 10 (filtered from 13) |
| **Train samples** | 3,087 (80%) |
| **Test samples** | 772 (20%) |
| **Features** | 505 (500 TF-IDF + 5 numeric) |

### Model Performance
| Model | Accuracy | Precision (macro) | Recall (macro) | F1-Score (macro) |
|-------|----------|-------------------|----------------|------------------|
| Random Forest | 0.69 | ~0.77 | ~0.65 | ~0.70 |
| **XGBoost** âœ… | **0.77** | **~0.81** | **~0.71** | **~0.75** |

### Per-Category Performance (XGBoost)
| Category | Precision | Recall | F1-Score | Support (Test) |
|----------|-----------|--------|----------|----------------|
| Other | 0.65 | 0.85 | 0.74 | 323 |
| Backend Developer | 0.82 | 0.75 | 0.78 | 75 |
| Fullstack Developer | 0.78 | 0.68 | 0.73 | 69 |
| QA/Tester | 0.88 | 0.72 | 0.79 | 68 |
| Mobile Developer | 0.85 | 0.73 | 0.79 | 61 |
| Frontend Developer | 0.79 | 0.71 | 0.75 | 58 |
| Software Engineer | 0.72 | 0.60 | 0.65 | 50 |
| Business Analyst | 0.83 | 0.75 | 0.79 | 37 |
| DevOps Engineer | 0.90 | 0.65 | 0.76 | 17 |
| Data Engineer | 0.88 | 0.70 | 0.78 | 14 |

**Best Categories** (F1 > 0.78):
- âœ… QA/Tester: 0.79
- âœ… Mobile Developer: 0.79
- âœ… Business Analyst: 0.79
- âœ… Backend Developer: 0.78
- âœ… Data Engineer: 0.78

**Challenging Categories** (F1 < 0.75):
- âš ï¸ Software Engineer: 0.65 (too generic, overlaps with other categories)
- âš ï¸ Fullstack Developer: 0.73 (overlaps with Backend/Frontend)
- âš ï¸ Other: 0.74 (catch-all, high support but low precision)

---

## ğŸ” Error Analysis

### Common Misclassifications

**1. Software Engineer â†” Backend/Frontend/Fullstack**
- **Problem**: "Software Engineer" is generic, overlaps with specific categories
- **Example**: "Software Engineer (Python)" â†’ Predicted as Backend (may be correct!)
- **Impact**: Low recall for Software Engineer (60%)

**2. Fullstack â†” Backend + Frontend**
- **Problem**: Fullstack jobs mention both backend and frontend skills
- **Example**: "Fullstack Developer (React + Node)" â†’ May predict Backend (Node keyword strong)
- **Impact**: Medium recall for Fullstack (68%)

**3. Other â† Many categories**
- **Problem**: "Other" catches jobs with non-standard titles
- **Example**: Vietnamese titles, non-tech roles â†’ Predicted as "Other"
- **Impact**: High recall for "Other" (85%), but steals samples from other categories

**4. DevOps â†” Backend**
- **Problem**: DevOps jobs often mention backend skills (Docker, Kubernetes, Python)
- **Example**: "DevOps Engineer (AWS, Docker, Python)" â†’ May predict Backend
- **Impact**: Low recall for DevOps (65%)

**5. Data Engineer â†” Backend**
- **Problem**: Data Engineer jobs mention backend skills (Python, SQL, ETL)
- **Example**: "Data Engineer (Python, Spark, SQL)" â†’ May predict Backend
- **Impact**: Medium recall for Data Engineer (70%)

### Root Causes

**1. Class Imbalance**
- "Other" (41.8%) dominates â†’ Model biased toward predicting "Other"
- Small classes (DevOps 2.3%, Data Engineer 1.8%) â†’ Harder to learn patterns

**2. Overlapping Keywords**
- Backend, Fullstack, DevOps, Data Engineer all mention: Python, SQL, Docker
- Model struggles to differentiate without context

**3. Generic Terms**
- "Software Engineer" applies to all categories â†’ Hard to classify

**4. Vietnamese Titles**
- Many Vietnamese job titles â†’ Classified as "Other" (no English keywords)
- TF-IDF may not capture Vietnamese terms well

---

## ğŸš€ Planned Improvements

### **NhÃ³m 1 â€“ Baseline & Evaluation Enhancement**

#### Improvement 1.1: Add Simple Baseline Models
**Goal**: Compare XGBoost against simpler, faster models.

**Models to Add**:
1. **Logistic Regression**
   - Linear model, fast training
   - Good for text classification
   - Expected accuracy: 60-65%
   
2. **Linear SVM** (Support Vector Machine)
   - Linear kernel, efficient for sparse data
   - Strong baseline for text
   - Expected accuracy: 65-70%

**Why?**:
- Validate that complex models (XGBoost) are necessary
- Baselines are faster for inference (production-friendly)
- If Logistic Regression â‰ˆ XGBoost â†’ Use simpler model

**Implementation**:
```python
from sklearn.linear_model import LogisticRegression
from sklearn.svm import LinearSVC

# Logistic Regression baseline
lr_model = LogisticRegression(max_iter=1000, random_state=42)
lr_model.fit(X_train, y_train)
lr_acc = accuracy_score(y_test, lr_model.predict(X_test))

# Linear SVM baseline
svm_model = LinearSVC(max_iter=1000, random_state=42)
svm_model.fit(X_train, y_train)
svm_acc = accuracy_score(y_test, svm_model.predict(X_test))

print(f'Logistic Regression: {lr_acc:.4f}')
print(f'Linear SVM: {svm_acc:.4f}')
print(f'XGBoost: {xgb_acc:.4f}')
```

**Success Criteria**:
- âœ… Logistic Regression: 60-65% (reasonable baseline)
- âœ… Linear SVM: 65-70% (strong baseline)
- âœ… XGBoost: 75-80% (best model)
- âŒ If Logistic Regression > 75% â†’ XGBoost may be overkill

---

#### Improvement 1.2: Expand Evaluation Metrics
**Goal**: Move beyond accuracy to capture per-class performance.

**Current Metrics**:
- âœ… Accuracy (overall correctness)
- âœ… Per-class Precision, Recall, F1-Score

**Metrics to Add**:
1. **Macro-Averaged F1-Score**
   - Average F1 across all classes (equal weight)
   - Better for imbalanced datasets
   - Formula: `(F1_class1 + F1_class2 + ... + F1_class10) / 10`
   
2. **Macro-Averaged Precision**
   - Average precision across all classes
   - Measures false positive control per class
   
3. **Macro-Averaged Recall**
   - Average recall across all classes
   - Measures false negative control per class

**Why?**:
- Accuracy is misleading with imbalance (predicting "Other" always â†’ 41.8% accuracy)
- Macro metrics give equal importance to all classes (not biased by majority)

**Implementation**:
```python
from sklearn.metrics import precision_score, recall_score, f1_score

# Macro-averaged metrics
macro_precision = precision_score(y_test, y_pred, average='macro')
macro_recall = recall_score(y_test, y_pred, average='macro')
macro_f1 = f1_score(y_test, y_pred, average='macro')

print(f'Macro Precision: {macro_precision:.4f}')
print(f'Macro Recall: {macro_recall:.4f}')
print(f'Macro F1-Score: {macro_f1:.4f}')
```

**Success Criteria**:
- âœ… Macro F1 > 0.70 (balanced performance across all classes)
- âœ… Macro Precision > 0.75 (low false positives)
- âœ… Macro Recall > 0.65 (finding most samples per class)

---

### **NhÃ³m 2 â€“ Imbalance & Error Analysis**

#### Improvement 2.1: Address Class Imbalance
**Problem**: "Other" category dominates (41.8%), causing bias.

**Impact on Model**:
- Model predicts "Other" too often (high recall 85%, low precision 65%)
- Small classes (DevOps 2.3%, Data Engineer 1.8%) have low recall (65-70%)
- Majority class steals predictions from minority classes

**Proposed Solutions**:

**Option 1: Undersample Majority Class**
```python
from imblearn.under_sampling import RandomUnderSampler

# Reduce "Other" to match 2nd largest class (~400 samples)
rus = RandomUnderSampler(sampling_strategy={'Other': 400}, random_state=42)
X_resampled, y_resampled = rus.fit_resample(X_train, y_train)

# Retrain model on balanced data
model.fit(X_resampled, y_resampled)
```

**Pros**:
- âœ… Balanced training data
- âœ… Model learns all classes equally

**Cons**:
- âŒ Lose data (discard ~1,200 "Other" samples)
- âŒ Model may not reflect real distribution (41.8% "Other" in reality)

---

**Option 2: Adjust Class Weights**
```python
# XGBoost with class weights
from sklearn.utils.class_weight import compute_class_weight

class_weights = compute_class_weight('balanced', classes=np.unique(y_train), y=y_train)
sample_weights = np.array([class_weights[y] for y in y_train])

xgb_model = XGBClassifier(n_estimators=100, random_state=42)
xgb_model.fit(X_train, y_train, sample_weight=sample_weights)
```

**Pros**:
- âœ… Keep all data
- âœ… Penalize errors on minority classes more

**Cons**:
- âŒ May reduce accuracy on "Other" (trade-off)
- âŒ Hyperparameter tuning needed

---

**Option 3: Oversample Minority Classes** (SMOTE)
```python
from imblearn.over_sampling import SMOTE

# Oversample small classes to match "Other"
smote = SMOTE(random_state=42)
X_resampled, y_resampled = smote.fit_resample(X_train, y_train)

model.fit(X_resampled, y_resampled)
```

**Pros**:
- âœ… No data loss
- âœ… Balanced training data

**Cons**:
- âŒ Synthetic samples (may not be realistic)
- âŒ Increased training time (more samples)

---

**Recommended Strategy**:
1. Start with **Option 2 (Class Weights)** - Simplest, no data loss
2. If F1 for small classes improves â†’ Keep
3. If overall accuracy drops significantly â†’ Try **Option 1 (Undersampling)**

**Success Criteria**:
- âœ… Improve F1 for DevOps (from 0.76 to >0.80)
- âœ… Improve F1 for Data Engineer (from 0.78 to >0.82)
- âœ… Reduce "Other" recall (from 85% to ~75%) without hurting other classes
- âœ… Overall macro F1 improves (from ~0.75 to >0.77)

---

#### Improvement 2.2: Detailed Error Analysis
**Goal**: Understand misclassifications to guide improvements.

**Analysis Steps**:

**Step 1: Extract Misclassified Jobs**
```python
# Get indices of misclassified samples
misclassified_idx = np.where(y_test != y_pred)[0]

# Create DataFrame of errors
errors_df = df_ml.iloc[test_idx[misclassified_idx]].copy()
errors_df['true_category'] = le_target.inverse_transform(y_test[misclassified_idx])
errors_df['predicted_category'] = le_target.inverse_transform(y_pred[misclassified_idx])

# Most common error patterns
error_pairs = errors_df.groupby(['true_category', 'predicted_category']).size().sort_values(ascending=False)
print(error_pairs.head(10))
```

**Step 2: Analyze Fullstack Developer Errors** (from VERIFICATION_CHECKLIST)
- **Known issue**: Fullstack is challenging category
- **Questions**:
  - How many Fullstack jobs misclassified as Backend?
  - How many as Frontend?
  - What keywords are present in misclassified Fullstack jobs?

```python
# Filter Fullstack errors
fullstack_errors = errors_df[errors_df['true_category'] == 'Fullstack Developer']

print(f'Total Fullstack errors: {len(fullstack_errors)}')
print(f'Predicted as Backend: {(fullstack_errors["predicted_category"] == "Backend Developer").sum()}')
print(f'Predicted as Frontend: {(fullstack_errors["predicted_category"] == "Frontend Developer").sum()}')

# Analyze keywords
from collections import Counter
fullstack_skills = fullstack_errors['skills'].dropna().str.split('|')
all_skills = [skill.strip().lower() for sublist in fullstack_skills for skill in sublist]
skill_counts = Counter(all_skills).most_common(20)
print('Top skills in misclassified Fullstack jobs:', skill_counts)
```

**Step 3: Confusion Analysis**
```python
# Find most confused pairs
for i, row in error_pairs.head(10).items():
    true_cat, pred_cat = i
    count = row
    print(f'{true_cat} â†’ {pred_cat}: {count} errors')
    
    # Sample errors
    sample = errors_df[(errors_df['true_category'] == true_cat) & 
                        (errors_df['predicted_category'] == pred_cat)].head(3)
    for _, job in sample.iterrows():
        print(f'  Title: {job["job_title"]}')
        print(f'  Skills: {job["skills"][:100]}...')
```

**Expected Findings**:
1. **Fullstack â†’ Backend** (high errors)
   - Reason: Fullstack jobs emphasize backend skills (Python, Node, databases)
   - Fix: Add bigrams "full stack", "fullstack" to boost weight
   
2. **Software Engineer â†’ Backend/Frontend** (high errors)
   - Reason: Generic title, relies heavily on description keywords
   - Fix: May need separate "Software Engineer" detector (rule-based)
   
3. **DevOps â†’ Backend** (medium errors)
   - Reason: Both use Python, Docker, cloud skills
   - Fix: Boost DevOps-specific keywords (Kubernetes, Jenkins, CI/CD, Terraform)

**Success Criteria**:
- âœ… Document top 10 error patterns
- âœ… Identify keywords causing confusion
- âœ… Propose keyword boosting or rule-based overrides
- âœ… Reduce Fullstack â†’ Backend errors by 20%

---

### **NhÃ³m 3 â€“ Feature & Model Enhancement**

#### Improvement 3.1: Add Skill Count Feature
**Goal**: Capture number of skills as a feature (not just which skills).

**Hypothesis**:
- Senior jobs require more skills (broader expertise)
- Junior jobs have fewer skills (focused learning)
- Skill count may help distinguish levels and categories

**Implementation**:
```python
# Count number of skills per job
df_ml['skill_count'] = df_ml['skills'].fillna('').str.split('|').str.len()

# Add to numeric features
feature_cols = ['level_encoded', 'city_encoded', 'has_salary', 
                'title_length', 'desc_length', 'skill_count']  # NEW
X_additional = df_ml[feature_cols].values
```

**Expected Impact**:
- Backend/Fullstack/DevOps: Higher skill counts (6-10 skills)
- QA/Business Analyst: Lower skill counts (3-5 skills)
- May help distinguish between Backend and DevOps (DevOps = more infra skills)

**Success Criteria**:
- âœ… Skill count correlates with category (ANOVA test)
- âœ… Model accuracy improves by 1-2% (0.77 â†’ 0.78-0.79)

---

#### Improvement 3.2: Group Skills by Stack
**Goal**: Create high-level skill categories (web, data, cloud, etc.).

**Skill Stacks**:
```python
skill_groups = {
    'web_frontend': ['react', 'vue', 'angular', 'javascript', 'typescript', 'html', 'css'],
    'web_backend': ['python', 'java', 'node', 'php', 'django', 'spring', 'laravel'],
    'mobile': ['ios', 'android', 'swift', 'kotlin', 'react native', 'flutter'],
    'data': ['sql', 'postgresql', 'mysql', 'mongodb', 'redis', 'elasticsearch'],
    'cloud': ['aws', 'azure', 'gcp', 'docker', 'kubernetes'],
    'devops': ['jenkins', 'gitlab', 'ci/cd', 'terraform', 'ansible'],
    'data_engineering': ['spark', 'kafka', 'airflow', 'etl', 'hadoop'],
    'testing': ['selenium', 'jira', 'postman', 'automation']
}

# Count skills per stack
def count_stack_skills(skills_str, stack_keywords):
    if pd.isna(skills_str):
        return 0
    skills = skills_str.lower().split('|')
    return sum(1 for skill in skills if any(kw in skill for kw in stack_keywords))

for stack_name, keywords in skill_groups.items():
    df_ml[f'stack_{stack_name}'] = df_ml['skills'].apply(
        lambda x: count_stack_skills(x, keywords)
    )

# Add to features
stack_cols = [f'stack_{name}' for name in skill_groups.keys()]
feature_cols.extend(stack_cols)
```

**Expected Impact**:
- Frontend: High `stack_web_frontend`, low `stack_web_backend`
- Backend: High `stack_web_backend`, medium `stack_data`
- DevOps: High `stack_cloud`, high `stack_devops`
- Data Engineer: High `stack_data_engineering`, high `stack_data`

**Success Criteria**:
- âœ… Stack features have high feature importance (>1% in XGBoost)
- âœ… Model accuracy improves by 2-3% (0.77 â†’ 0.79-0.80)
- âœ… Reduce confusion between Backend/DevOps/Data Engineer

---

#### Improvement 3.3: Try Pre-trained Embeddings (PhoBERT/BERT)
**Goal**: Replace TF-IDF with contextual embeddings from language models.

**Current Limitation (TF-IDF)**:
- Bag-of-words model (no word order, no context)
- "Backend Engineer" and "Engineer Backend" treated differently
- No understanding of synonyms (e.g., "developer" â‰ˆ "engineer")

**Proposed Solution (BERT/PhoBERT)**:
- Use pre-trained embeddings (768-dim vectors)
- Captures semantic meaning and context
- PhoBERT: Vietnamese + English support

**Implementation (High-Level)**:
```python
from transformers import AutoTokenizer, AutoModel
import torch

# Load PhoBERT
tokenizer = AutoTokenizer.from_pretrained("vinai/phobert-base")
model = AutoModel.from_pretrained("vinai/phobert-base")

# Encode text
def encode_text(text):
    inputs = tokenizer(text, return_tensors="pt", truncation=True, max_length=512)
    with torch.no_grad():
        outputs = model(**inputs)
    # Use [CLS] token embedding (768-dim)
    return outputs.last_hidden_state[:, 0, :].numpy()

# Apply to dataset
df_ml['text_embedding'] = df_ml['text'].apply(encode_text)

# Concatenate with numeric features
X_bert = np.vstack(df_ml['text_embedding'].values)  # (3,859 Ã— 768)
X_combined = np.hstack([X_bert, X_additional])      # (3,859 Ã— 773)
```

**Pros**:
- âœ… Better semantic understanding
- âœ… Handles Vietnamese titles better
- âœ… May improve accuracy by 5-10% (0.77 â†’ 0.82-0.87)

**Cons**:
- âŒ Much slower (requires GPU for inference)
- âŒ Larger model size (~500 MB for PhoBERT)
- âŒ Requires transformers library (heavier dependencies)

**Success Criteria**:
- âœ… BERT model accuracy > 0.82 (5% improvement)
- âœ… Vietnamese job titles classified correctly (improve "Other" precision)
- âœ… Inference time acceptable (<1 second per job on GPU)

**Timeline**: Future work (Phase 2 of ML pipeline)

---

## ğŸ“Š Success Metrics Summary

### Baseline Improvements (NhÃ³m 1)
| Improvement | Current | Target | Success? |
|-------------|---------|--------|----------|
| Logistic Regression baseline | N/A | 60-65% | âœ… If achieved |
| Linear SVM baseline | N/A | 65-70% | âœ… If achieved |
| Macro F1-Score tracking | ~0.75 | >0.75 | âœ… If maintained |

### Imbalance Solutions (NhÃ³m 2)
| Improvement | Current | Target | Success? |
|-------------|---------|--------|----------|
| DevOps F1-Score | 0.76 | >0.80 | âœ… If achieved |
| Data Engineer F1-Score | 0.78 | >0.82 | âœ… If achieved |
| "Other" recall | 0.85 | ~0.75 | âœ… If reduced without hurting others |
| Macro F1 improvement | ~0.75 | >0.77 | âœ… If achieved |
| Fullstack â†’ Backend errors | Baseline | -20% | âœ… If reduced |

### Feature Enhancements (NhÃ³m 3)
| Improvement | Current | Target | Success? |
|-------------|---------|--------|----------|
| Skill count feature | N/A | +1-2% acc | âœ… If accuracy improves |
| Skill stack features | N/A | +2-3% acc | âœ… If accuracy improves |
| BERT embeddings | 0.77 | >0.82 | âœ… If 5%+ improvement |

---

## ğŸ”„ Implementation Priority

### Phase 1 (Immediate - Next sprint)
1. âœ… **Add baseline models** (Logistic Regression, Linear SVM) - 1 hour
2. âœ… **Expand evaluation metrics** (Macro F1, Precision, Recall) - 30 min
3. âœ… **Error analysis** (Extract misclassified jobs, analyze patterns) - 2 hours

### Phase 2 (Short-term - 1-2 weeks)
1. âœ… **Address class imbalance** (Try class weights first) - 2 hours
2. âœ… **Add skill count feature** - 1 hour
3. âœ… **Evaluate impact on Fullstack/DevOps categories** - 1 hour

### Phase 3 (Medium-term - 1 month)
1. âœ… **Group skills by stack** - 3 hours
2. âœ… **Retrain with new features** - 1 hour
3. âœ… **Compare with baseline** - 1 hour

### Phase 4 (Long-term - Future work)
1. ğŸ“… **Research PhoBERT/BERT integration** - 1 week
2. ğŸ“… **Implement BERT pipeline** - 2 weeks
3. ğŸ“… **Benchmark BERT vs TF-IDF** - 3 days

---

## ğŸ”— Cross-References

### Related Documentation
- **Schema**: `docs/schema.md` - Column definitions (job_title, job_description, skills, job_level, city)
- **Categorization**: `docs/categorization_rules.md` - Baseline rules for categories (compare with ML predictions)
- **Pipeline**: `docs/pipeline_overview.md` - Full data pipeline (Steps 1-15)
- **Master Data**: `data/final/jobs_master.csv` - Input dataset (3,985 jobs)
- **Notebook**: `vietnam_it_jobs_merge_analysis.ipynb` - Implementation code (Steps 11-15)

### Key Files
- **Model Package**: `data/final/best_model.pkl` - Trained XGBoost + encoders
- **Train Data**: `data/final/jobs_master.csv` - Filtered to 3,859 jobs
- **README**: `README.md` - Project overview, ML section (Steps 11-15)

---

## ğŸ“ Notes & Considerations

### Assumptions
1. **Text quality**: Assumes job descriptions are in English or mixed English/Vietnamese
2. **Skill format**: Assumes skills are pipe-separated (e.g., `python|django|sql`)
3. **Category labels**: Assumes ground truth categories are correct (no mislabeling)

### Limitations
1. **TF-IDF**: No semantic understanding (bag-of-words)
2. **Class imbalance**: "Other" dominates, biases model
3. **Generic categories**: "Software Engineer", "Other" hard to classify
4. **Vietnamese titles**: Not well-captured by English stopwords and TF-IDF

### Future Considerations
1. **Multi-label classification**: Allow jobs to have multiple categories (e.g., Backend + DevOps)
2. **Hierarchical classification**: Backend â†’ Backend (Python) vs Backend (Java)
3. **Confidence scores**: Return prediction probability (e.g., 85% Backend, 15% DevOps)
4. **Active learning**: Ask users to label uncertain predictions to improve model

---

## âœ… Checklist - ML Pipeline Complete

### Implementation
- [x] Filter categories (â‰¥50 samples) â†’ 3,859 jobs
- [x] Build text feature (title + description + skills)
- [x] TF-IDF vectorization (500 features, bigrams)
- [x] Add numeric features (level, city, lengths)
- [x] Train/test split (80/20, stratified)
- [x] Train Random Forest (69% accuracy)
- [x] Train XGBoost (77% accuracy) âœ… Best
- [x] Evaluate with classification report
- [x] Generate confusion matrix
- [x] Save best model package (best_model.pkl)

### Documentation
- [x] Document all 8 steps with code examples
- [x] Explain TF-IDF parameters (max_features, ngram_range, stopwords)
- [x] Document numeric features (5 features)
- [x] Show per-category performance table
- [x] Identify error patterns and root causes
- [x] Propose 3 improvement groups (Baseline, Imbalance, Features)
- [x] Define success criteria for each improvement
- [x] Create implementation priority roadmap

### Cross-References
- [x] Link to schema.md, categorization_rules.md, pipeline_overview.md
- [x] Reference README.md ML section
- [x] Point to notebook implementation (Steps 11-15)

---

**Status**: âœ… **ML Pipeline Documentation HOÃ€N THÃ€NH**  
**Next Steps**: Implement improvements (Phase 1 â†’ Phase 4)  
**Best Model**: XGBoost (77% accuracy, 0.75 macro F1)  
**Key Challenge**: Class imbalance ("Other" 41.8%, Data Engineer 1.8%)

---

## ğŸ“ Key Takeaways

### What Works Well
1. âœ… **XGBoost outperforms Random Forest** by 8% (77% vs 69%)
2. âœ… **TF-IDF captures technical keywords** (python, java, react, etc.)
3. âœ… **High precision categories** (DevOps 90%, QA 88%, Data Engineer 88%)
4. âœ… **96.8% data retained** after filtering (only 3 categories excluded)

### What Needs Improvement
1. âš ï¸ **Class imbalance** - "Other" dominates (41.8%), biases predictions
2. âš ï¸ **Generic categories** - Software Engineer (F1 = 0.65), Fullstack (F1 = 0.73)
3. âš ï¸ **Overlapping keywords** - Backend/DevOps/Data Engineer all use Python, SQL, Docker
4. âš ï¸ **Vietnamese titles** - Not well-captured by English TF-IDF

### Recommended Next Steps
1. ğŸ¯ **Priority 1**: Add baseline models + macro metrics (validate current approach)
2. ğŸ¯ **Priority 2**: Address class imbalance (class weights or undersampling)
3. ğŸ¯ **Priority 3**: Error analysis (understand Fullstack/Software Engineer confusion)
4. ğŸ¯ **Priority 4**: Add skill count + stack features (improve category separation)
5. ğŸ“… **Future**: Explore BERT/PhoBERT for semantic understanding

---

**Ready for Phase 1 improvements!** ğŸš€
